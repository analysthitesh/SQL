{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.1 Supervised or Unsupervised Task\n",
    "\n",
    "Assuming that data mining techniques are to be used in the following cases, identify whether the task required is supervised or unsupervised learning.\n",
    "\n",
    "__2.1.a.__ Deciding whether to issue a loan to an applicant based on demographic and financial data (with reference to a database of similar data on prior customers).\n",
    "\n",
    "__Answer:__ This is supervised learning, because the database includes information on whether the loan was approved or not.\n",
    "\n",
    "__2.1.b.__ In an online bookstore, making recommendations to customers concerning additional items to buy based on the buying patterns in prior transactions.\n",
    "\n",
    "__Answer:__ This is unsupervised learning, because there is no apparent outcome (e.g., whether the recommendation was adopted or not).\n",
    "\n",
    "__2.1.c.__ Identifying a network data packet as dangerous (virus, hacker attack) based on comparison to other packets whose threat status is known.\n",
    "\n",
    "__Answer:__ This is supervised learning, because for the other packets the status is known.\n",
    "\n",
    "__2.1.d.__ Identifying segments of similar customers.\n",
    "\n",
    "__Answer:__ This is unsupervised learning because there is no known outcome (though once you use unsupervised learning to identify segments, you could use supervised learning to classify new customers into those segments).\n",
    "\n",
    "__2.1.e.__ Predicting whether a company will go bankrupt based on comparing its financial data to those of similar bankrupt and nonbankrupt firms.\n",
    "\n",
    "__Answer:__ This is supervised learning, because the status of the similar firms is known.\n",
    "\n",
    "__2.1.f.__ Estimating the repair time required for an aircraft based on a trouble ticket.\n",
    "\n",
    "__Answer:__ This is supervised learning, because there is likely to be knowledge of actual (historic) repair times of similar repairs.\n",
    "\n",
    "__2.1.g.__ Automated sorting of mail by zip code scanning.\n",
    "\n",
    "__Answer:__ This is supervised learning, as there is likely to be knowledge about whether the sorting was correct in previous mail sorting.\n",
    "\n",
    "__2.1.h.__ Printing of custom discount coupons at the conclusion of a grocery store checkout based on what you just bought and what others have bought previously.\n",
    "\n",
    "__Answer:__ This is unsupervised learning, if we assume that we do not know what will be purchased in the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.5\n",
    "Using the concept of overfitting, explain why when a model is fit to training data, zero error with those data is not necessarily good.\n",
    "\n",
    "__Answer:__\n",
    "Overfitting occurs when the model captures not only the generalizable pattern in the data, but also the error. When we split the data into training and validation sets, we assume that the same pattern (if there is a pattern) exists in both, and that they differ only in the error that they contain. An absurd and false model may fit perfectly (on training data set) if the model has enough complexity. Therefore we may get zero error for such a model using the training dataset. Such a model, however, is not likely to give useful results on the validation data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.8\n",
    "Normalize the data in Table 2.18, showing calculations.\n",
    "\n",
    "__Answer:__\n",
    "Normalization of a measurement is obtained by subtracting the (column) average from each measurement and dividing the difference by the (column) standard deviation.\n",
    "\n",
    "For variable Age (years):\n",
    "Mean =44.66667 and Standard deviation (std) = 14.97554\n",
    "\n",
    "For variable Income ($):\n",
    "Mean=98.66667 and Standard deviation (std) = 62.86706\n",
    "\n",
    "For normalizing age for observation # 1 (Here age = 25):\n",
    "After subtracting the average and dividing by standard deviation, the normalized age = -1.438596.\n",
    "\n",
    "For normalizing income for observation # 1 (Here Income = 49000):\n",
    "After subtracting the average income and dividing by standard deviation, the normalized income = -0.865431.\n",
    "\n",
    "Let's normalize the data using sklearn's preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required functionality for this assignment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  Income\n",
      "0   25   49000\n",
      "1   56  156000\n",
      "2   65   99000\n",
      "3   32  192000\n",
      "4   41   39000\n",
      "5   49   57000\n"
     ]
    }
   ],
   "source": [
    "# import the required functionality for this problem\n",
    "\n",
    "\n",
    "# create a data frame\n",
    "df = pd.DataFrame({\n",
    "    'Age': [25, 56, 65, 32, 41, 49],\n",
    "    'Income': [49000, 156000, 99000, 192000, 39000, 57000]\n",
    "})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Age    Income\n",
      "0 -1.438597 -0.865431\n",
      "1  0.829022  0.999021\n",
      "2  1.487363  0.005808\n",
      "3 -0.926554  1.626314\n",
      "4 -0.268213 -1.039679\n",
      "5  0.316979 -0.726033\n"
     ]
    }
   ],
   "source": [
    "# normalize the data\n",
    "scaler = StandardScaler()\n",
    "df_norm = pd.DataFrame(scaler.fit_transform(df), index=df.index, columns=df.columns)\n",
    "print(df_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.10\n",
    "Two models are applied to a dataset that has been partitioned. Model A is considerably more accurate than model B on the training data, but slightly less accurate than model B on the validation data. Which model are you more likely to consider for final deployment?\n",
    "\n",
    "__Answer:__\n",
    "We prefer the model with the lowest error on the validation data. Model A might be overfitting the training data. We would therefore select model B for deployment on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.11\n",
    "The dataset *ToyotaCorolla.csv* contains data on used cars on sale during the late summer of 2004 in the Netherlands. It has 1436 records containing details on 38 attributes, including *Price, Age, Kilometers, HP,* and other specifications. \n",
    "\n",
    "We plan to analyze the data using various data mining techniques described in future chapters. Prepare the data for use as follows:\n",
    "\n",
    "__2.11.a.__ The dataset has two categorical attributes, Fuel Type and Color. Describe how you would convert these to binary variables. Confirm this using __pandas__ methods to transform categorical data into dummies. What would you do with the variable _Model_?\n",
    "\n",
    "__Answer:__\n",
    "The categorical fuel_type variable has three categories: petrol, diesel and CNG. To convert these variables into dummy variables, we use only two variables (here we use Petrol and Diesel, but a different pair can also be chosen). The binary variable Petrol gets the value 1 if Fuel Type=Petrol and otherwise it gets the value 0. The binary variable Diesel gets the value 1 if Fuel Type=Diesel and otherwise it gets the value 0. CNG is the \"reference category.\" If Fuel type is CNG, both binary variables take the value 0.\n",
    "\n",
    "Similarly, the variable _Color_ is converted to dummy variables. It resulted in 10 dummy variables with \"Color_Beige\" as the reference category.\n",
    "\n",
    "Making dummies of all the categories in _Model_ would make for a lot of predictor variables.  Some preliminary exploration should be done first, to see if there are a small number of models that account for most cases.  The analysis could be confined initially to those to see how important Model is as a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "toyota_df = pd.read_csv('ToyotaCorolla.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id                                          Model  Price  Age_08_04  \\\n",
      "0   1  TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  13500         23   \n",
      "1   2  TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  13750         23   \n",
      "2   3  TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  13950         24   \n",
      "3   4  TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  14950         26   \n",
      "4   5    TOYOTA Corolla 2.0 D4D HATCHB SOL 2/3-Doors  13750         30   \n",
      "\n",
      "   Mfg_Month  Mfg_Year     KM Fuel_Type  HP  Met_Color  ... Powered_Windows  \\\n",
      "0         10      2002  46986    Diesel  90          1  ...               1   \n",
      "1         10      2002  72937    Diesel  90          1  ...               0   \n",
      "2          9      2002  41711    Diesel  90          1  ...               0   \n",
      "3          7      2002  48000    Diesel  90          0  ...               0   \n",
      "4          3      2002  38500    Diesel  90          0  ...               1   \n",
      "\n",
      "   Power_Steering  Radio  Mistlamps  Sport_Model  Backseat_Divider  \\\n",
      "0               1      0          0            0                 1   \n",
      "1               1      0          0            0                 1   \n",
      "2               1      0          0            0                 1   \n",
      "3               1      0          0            0                 1   \n",
      "4               1      0          1            0                 1   \n",
      "\n",
      "   Metallic_Rim  Radio_cassette  Parking_Assistant  Tow_Bar  \n",
      "0             0               0                  0        0  \n",
      "1             0               0                  0        0  \n",
      "2             0               0                  0        0  \n",
      "3             0               0                  0        0  \n",
      "4             0               0                  0        0  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "# review first few records\n",
    "print(toyota_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Blue', 'Silver', 'Black', 'White', 'Grey', 'Red', 'Green',\n",
       "       'Yellow', 'Violet', 'Beige'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check unique values for the variable Color\n",
    "toyota_df.Color.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create dummy variables for categorical variables, ignore the variable Model\n",
    "toyotadummies_df = pd.get_dummies(toyota_df.iloc[:,3:39], prefix_sep='_', drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Age_08_04', 'Mfg_Month', 'Mfg_Year', 'KM', 'HP', 'Met_Color',\n",
      "       'Automatic', 'CC', 'Doors', 'Cylinders', 'Gears', 'Quarterly_Tax',\n",
      "       'Weight', 'Mfr_Guarantee', 'BOVAG_Guarantee', 'Guarantee_Period', 'ABS',\n",
      "       'Airbag_1', 'Airbag_2', 'Airco', 'Automatic_airco', 'Boardcomputer',\n",
      "       'CD_Player', 'Central_Lock', 'Powered_Windows', 'Power_Steering',\n",
      "       'Radio', 'Mistlamps', 'Sport_Model', 'Backseat_Divider', 'Metallic_Rim',\n",
      "       'Radio_cassette', 'Parking_Assistant', 'Tow_Bar', 'Fuel_Type_Diesel',\n",
      "       'Fuel_Type_Petrol', 'Color_Black', 'Color_Blue', 'Color_Green',\n",
      "       'Color_Grey', 'Color_Red', 'Color_Silver', 'Color_Violet',\n",
      "       'Color_White', 'Color_Yellow'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# print column/variable names\n",
    "print(toyotadummies_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fuel_Type_Diesel  Fuel_Type_Petrol  Color_Black  Color_Blue  Color_Green\n",
      "0                 1                 0            0           1            0\n",
      "1                 1                 0            0           0            0\n",
      "2                 1                 0            0           1            0\n",
      "3                 1                 0            1           0            0\n",
      "4                 1                 0            1           0            0\n"
     ]
    }
   ],
   "source": [
    "# review first few records in dummy variables\n",
    "print(toyotadummies_df.loc[:,'Fuel_Type_Diesel':'Color_Green'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.11.b.__ Prepare the dataset (as factored into dummies) for data mining techniques of supervised learning by creating partitions in Python. Select all the variables and use default values for the random seed and partitioning percentages for training (50%),\n",
    "validation (30%), and test (20%) sets. Describe the roles that these partitions will play in modeling.\n",
    "\n",
    "__Answer__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   :  (718, 39)\n",
      "Validation :  (430, 39)\n",
      "Test       :  (288, 39)\n"
     ]
    }
   ],
   "source": [
    "trainData, temp = train_test_split(toyota_df, test_size=0.5, random_state=1)\n",
    "validData, testData = train_test_split(temp, test_size=0.4, random_state=1)\n",
    "\n",
    "print('Training   : ', trainData.shape)\n",
    "print('Validation : ', validData.shape)\n",
    "print('Test       : ', testData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training dataset__\n",
    "\n",
    "The training dataset is used to train or build models. For example, in a linear regression, the training dataset is used to fit the linear regression model, i.e. to compute the regression coefficients. This is usually the largest partition.\n",
    "\n",
    "__Validation dataset__\n",
    "\n",
    "Once a model is built on training data, we assess the accuracy of the model on unseen data. For this, the model should be used on a dataset that was not used in the training process. In the validation data we know the actual value of the response variable, and can therefore examine the difference between the actual value and the predicted value to determine the error in prediction. Based on this performance, sometimes the validation dataset is used to tweak the model, or to choose between multiple fitted models.\n",
    "\n",
    "__Test dataset__\n",
    "\n",
    "The validation dataset is often used to select a model with minimum error. Testing that model on completely unseen data gives a realistic estimate of the performance of the model. When a model is finally chosen, its accuracy with the validation dataset is still an optimistic estimate of how it would perform with unseen data. This is because (1) the final model has come out as the winner among the competing models based on the fact that its accuracy with the validation dataset is highest, and/or (2) the validation set was used to help build one or more models. Thus, you need to set aside yet another portion of data, which is used neither in training nor in validation, which is called the test dataset. The accuracy of the model on the test data gives a realistic estimate of the performance of the model on completely unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
